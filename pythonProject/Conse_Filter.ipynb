{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-25T16:09:18.092841Z",
     "start_time": "2024-07-25T14:41:27.096739Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_all_column_names(file_list):\n",
    "    all_columns = set()\n",
    "    for file in tqdm(file_list, desc=\"Extracting column names\"):\n",
    "        df = pq.read_table(file).to_pandas()\n",
    "        all_columns.update(df.columns)\n",
    "    return list(all_columns)\n",
    "\n",
    "def count_same_values(file, aggregated_counts, all_columns):\n",
    "    df = pq.read_table(file).to_pandas()\n",
    "    \n",
    "    for col in all_columns:\n",
    "        if col in df.columns:\n",
    "            data = df[col].values\n",
    "            for t in range(2, len(data)):\n",
    "                if data[t-1] == data[t-2]:\n",
    "                    aggregated_counts[col]['same_2_previous'] += 1\n",
    "                    if data[t] == data[t-1]:\n",
    "                        aggregated_counts[col]['same_3_consecutive'] += 1\n",
    "\n",
    "def calculate_probabilities(aggregated_counts):\n",
    "    probabilities = {}\n",
    "    for col, count in aggregated_counts.items():\n",
    "        if count['same_2_previous'] > 0:\n",
    "            probabilities[col] = count['same_3_consecutive'] / count['same_2_previous']\n",
    "        else:\n",
    "            probabilities[col] = None  # No cases where t-1 == t-2\n",
    "    return probabilities\n",
    "\n",
    "def main():\n",
    "    # Directory containing the parquet files\n",
    "    directory = f'D:\\\\2min-resample\\MetaDataSeparation\\MetaData Filtered\\WO_RetT\\\\'\n",
    "    file_list = glob.glob(directory + '*.parquet')\n",
    "    # Step 1: Get all column names\n",
    "    all_columns = get_all_column_names(file_list)\n",
    "    \n",
    "    # Initialize the aggregated counts dictionary\n",
    "    aggregated_counts = {col: {'same_2_previous': 0, 'same_3_consecutive': 0} for col in all_columns}\n",
    "    \n",
    "    # Step 2: Count values for each file and aggregate\n",
    "    for file in tqdm(file_list, desc=\"Processing files\"):\n",
    "        count_same_values(file, aggregated_counts, all_columns)\n",
    "    \n",
    "    # Step 3: Calculate probabilities\n",
    "    probabilities = calculate_probabilities(aggregated_counts)\n",
    "\n",
    "    # Step 4: Create a DataFrame to display results\n",
    "    results = []\n",
    "    for col in all_columns:\n",
    "        same_2_previous = aggregated_counts[col]['same_2_previous']\n",
    "        same_3_consecutive = aggregated_counts[col]['same_3_consecutive']\n",
    "        prob = probabilities[col]\n",
    "        results.append([col, same_2_previous, same_3_consecutive, prob])\n",
    "\n",
    "    df_results = pd.DataFrame(results, columns=['Column', 'Same_2_Previous_Count', 'Same_3_Consecutive_Count', 'Conditional_Probability'])\n",
    "    df_results.to_csv(f'D:\\\\2min-resample\\MetaDataSeparation\\MetaData Filtered\\WO_RetT\\conse_counts.csv',index=False)\n",
    "    print(df_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting column names: 100%|██████████| 1699/1699 [01:42<00:00, 16.54it/s]\n",
      "Processing files: 100%|██████████| 1699/1699 [1:26:08<00:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Column  Same_2_Previous_Count  Same_3_Consecutive_Count  \\\n",
      "0                   PrimMaxT              140497018                 140493293   \n",
      "1                      Brand               11395200                  11395118   \n",
      "2                BurnNoStart              461384922                 442444309   \n",
      "3                    ChBlock                1995802                   1995794   \n",
      "4   HwTOutlet[degC](float32)              141063621                  98744424   \n",
      "..                       ...                    ...                       ...   \n",
      "72               ServBoilOpM                3136260                   3136238   \n",
      "73                     ServY                2566031                   2566013   \n",
      "74               ChViaSwitch              472613795                 467055774   \n",
      "75                  FlameCur              434189691                 422304907   \n",
      "76           ChimneySwActive              322032716                 322025392   \n",
      "\n",
      "    Conditional_Probability  \n",
      "0                  0.999973  \n",
      "1                  0.999993  \n",
      "2                  0.958948  \n",
      "3                  0.999996  \n",
      "4                  0.699999  \n",
      "..                      ...  \n",
      "72                 0.999993  \n",
      "73                 0.999993  \n",
      "74                 0.988240  \n",
      "75                 0.972628  \n",
      "76                 0.999977  \n",
      "\n",
      "[77 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T17:35:38.295422Z",
     "start_time": "2024-07-25T17:12:26.060017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def replace_long_sequences(df, column, max_length):\n",
    "    current_length = 1\n",
    "    start_index = 0\n",
    "    \n",
    "    data = df[column].values\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i-1]:\n",
    "            if current_length == 1:\n",
    "                start_index = i - 1\n",
    "            current_length += 1\n",
    "        else:\n",
    "            if current_length > max_length and data[i-1] != 0:\n",
    "                df.iloc[start_index:i, df.columns.get_loc(column)] = None\n",
    "            current_length = 1\n",
    "\n",
    "    # Check the last sequence\n",
    "    if current_length > max_length and data[-1] != 0:\n",
    "        df.iloc[start_index:len(data), df.columns.get_loc(column)] = None\n",
    "\n",
    "def process_file(file, max_lengths):\n",
    "    df = pq.read_table(file).to_pandas()\n",
    "\n",
    "    # Set 'datetime' column as index\n",
    "    if 'datetime[](datetime)' in df.columns:\n",
    "        df.set_index('datetime[](datetime)', inplace=True)\n",
    "    \n",
    "    for col, max_length in max_lengths.items():\n",
    "        if col in df.columns:\n",
    "            replace_long_sequences(df, col, max_length)\n",
    "\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Directory containing the parquet files\n",
    "    directory = f'D:\\\\2min-resample\\MetaDataSeparation\\MetaData Filtered\\WO_RetT/'\n",
    "    file_list = glob.glob(os.path.join(directory, '*.parquet'))\n",
    "\n",
    "    # Directory to save processed files\n",
    "    processed_directory = os.path.join(directory, 'processed_files_Version3')\n",
    "    os.makedirs(processed_directory, exist_ok=True)\n",
    "\n",
    "    # Read the CSV file containing the longest sequence lengths\n",
    "    max_lengths_df = pd.read_csv(f'D:\\\\2min-resample\\MetaDataSeparation\\MetaData Filtered\\With_RetT\\conse_counts.csv')\n",
    "    columns_of_interest = [\n",
    " \"RetT\", \"BoilOpTime\",\n",
    "        \"SafSensT\", \"HwOpTime\", \"BurnOpTime\", \n",
    "          \"GasValMain\", \"HwActive[bool](float32)\",\n",
    "        \"HwFlow[L/min](float32)\",  \"ActPow[%](float32)\", \n",
    "        \"HwTOutlet[degC](float32)\", \n",
    "        \"OutTemp\", \"SysPrimT\"\n",
    "    ]\n",
    "    max_lengths_df = max_lengths_df[max_lengths_df['Column'].isin(columns_of_interest)]\n",
    "    max_lengths = max_lengths_df.set_index('Column')['Max_Length'].to_dict()\n",
    "\n",
    "    # Process each file\n",
    "    for file in tqdm(file_list, desc=\"Processing files\"):\n",
    "        processed_df = process_file(file, max_lengths)\n",
    "        \n",
    "        # Generate processed file path\n",
    "        file_name = os.path.basename(file)\n",
    "        processed_file_path = os.path.join(processed_directory, file_name)\n",
    "        \n",
    "        # Save the processed DataFrame to the new parquet file in the processed directory\n",
    "        processed_df.to_parquet(processed_file_path, index=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "8c618f847b60c390",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1699/1699 [23:12<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
